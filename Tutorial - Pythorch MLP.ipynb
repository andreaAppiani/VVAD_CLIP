{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1ezZjzheLUyL1_Wf0D8U4nMfPOhVN_3cz","timestamp":1611327456074}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"2DtAgGFhCE_s"},"source":["## Addestramento di un **Percettrone Multistrato** (Perceptrone Multistrato, MLP, acronimo di Multilayer Perceptron) in **PyTorch**\n","\n","* Applicheremo l'MLP per un compito di classificazione: Riconoscimento delle Cifre (Dati Immagine)\n","* Applicheremo l'MLP per un compito di regressione: ??? (Dati Tabulari)\n","\n","[ENG]\n","* We will apply MLP for a classification task: Digit Recognition (Image Data)\n","* We will apply MLP for a regression task: ??? (Tabular Data)"]},{"cell_type":"markdown","metadata":{"id":"KbcIR4gIDlXX"},"source":["### Dataset and Dataloaders\n","\n","Prima di tutto, dobbiamo recuperare i dati di MNIST. Dobbiamo creare un **Dataset** e un **DataLoader** come abbiamo fatto nella lezione precedente. Dobbiamo creare i set di addestramento, validazione e test e un DataLoader per ognuno di essi.\n","\n","[ENG]\n","First we need to retrieve the MNIST dataset.\n","We need to create Dataset and DataLoader as we did in previous lesson.\n","We need to create train, validation and test sets and a loader for each of them.\n"]},{"cell_type":"code","metadata":{"id":"_OLqgNgCGO9F"},"source":["import torch\n","import torchvision.transforms as T\n","def get_data(batch_size, test_batch_size=256):\n","\n","  # Preparare le trasformazioni dei dati e poi combinarle sequenzialmente\n","  transform = list()\n","  # Converte un'immagine rappresentata come un array NumPy in un tensore PyTorch\n","  transform.append(T.ToTensor())\n","  # Sottrae la media specificata (mean=[0.5]) e quindi divide per la deviazione standard specificata (std=[0.5])\n","  # Porta tutti i valori del tensore nell'intervallo compreso tra -1 e 1.\n","  transform.append(T.Normalize(mean=[0.5], std=[0.5]))      # [ENG] Normalizes the Tensors between [-1, 1]\n","  # combina le trasformazioni definite in transform in una singola trasformazione composta.\n","  # In pratica, questo significa che quando applichi questa trasformazione composta a un'immagine, le due trasformazioni definite sopra verranno applicate in sequenza.\n","  transform = T.Compose(transform)\n","\n","  # Carica il dataset di addestramento\n","  full_training_data = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True)\n","  # './data':  è la directory in cui il dataset verrà scaricato. Se non esiste, verrà creata.\n","  # train=True: specifica che questo dataset è per l'addestramento, quindi caricherà le immagini e le etichette di addestramento.\n","  # transform=transform: rappresenta l'insieme di trasformazioni delle immagini da applicare a ciascuna immagine durante il caricamento\n","  # download=True: indica al codice di scaricare il dataset da Internet se non è già disponibile in locale\n","\n","  # Carica il dataset di test\n","  test_data = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True)\n","  # train=False: Nel caso del dataset MNIST, questa opzione indica il caricamento del dataset di test.\n","\n","  # Crea divisioni per il training e la validazione\n","  # il set di addestramento che riceve circa il 70% dei dati e il set di convalida che riceve il resto\n","  num_samples = len(full_training_data)\n","  training_samples = int(num_samples*0.7+1) #Aggiungendo 1 si assicura che almeno un campione venga allocato al set di addestramento.\n","  validation_samples = num_samples - training_samples\n","\n","  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n","  #torch.utils.data.random_split: è una funzione che suddivide casualmente un dataset in base alle dimensioni specificate per i set di addestramento e di convalida\n","\n","  # Inizializza i dataloader\n","  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=4)\n","  # shuffle=True:  il DataLoader mescola i dati di addestramento prima di suddividerli in batch\n","  # num_workers=4: utilizza 4 processi paralleli\n","  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, num_workers=4)\n","  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, num_workers=4)\n","\n","  return train_loader, val_loader, test_loader\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"40GUUErvGgfv"},"source":["### Definizione della rete\n","\n","<img src=\"https://miro.medium.com/v2/resize:fit:640/1*63sGPbvLLpvlD16hG1bvmA.gif\" width=\"450\"></br></br>\n","\n","Un MLP è composto da due componenti:\n","\n","**Fully-Connected Layers:** (Strati Completamente Connessi): Sono definiti come `torch.nn.Linear`.\n","\n","**Activation function:** (Funzione di attivazione): Tra i livelli dobbiamo inserire una funzione di attivazione non lineare. Sono possibili molte scelte, vedi [doc](https://pytorch.org/docs/stable/nn.html).\n","Pero ora, useremo una funzione sigmoide (`torch.nn.Sigmoid`).\n","\n","❗Non dimenticare che una rete deve estendere una classe `torch.nn.Module`.\n","\n","Questo significa che quando si definisce una rete neurale in PyTorch, è necessario creare una classe che erediti dalla classe `torch.nn.Module`. Questo perché `torch.nn.Module` fornisce molte funzionalità utili per la definizione delle reti neurali, come la gestione automatica dei parametri, il calcolo automatico dei gradienti e la possibilità di annidare più moduli all'interno di una rete. Quindi, quando si definisce una rete, è importante farla ereditare da `torch.nn.Module` per sfruttare queste funzionalità.\n","\n","\n","[ENG]\n","\n","### Network Definition\n","\n","A MultiLayer Perceptron is made of two components:\n","\n","**Fully-Connected Layers:** The fully-connected layers are defined as `torch.nn.Linear`. In practice a fully conected layer is an affine transformation:\n","\n","$$\n","y = x \\cdot W^T + b\n","$$\n","\n","with two learnable set of parameters: the *weight*  $W$ and the *bias* $b$. \\\\\n","When defining a linear layer, we have to specify the number of input features and the number of output features, which are the dimensions of $x$ and $y$ respecively.\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1hpqFqktOou8C4ZVOrJDiyYYG0b9PpPO2\" width=\"450\"></br></br>\n","\n","\n","**Activation function:** Between the layers we must put a non-linear activation. Multiple choices are possible, see [doc](https://pytorch.org/docs/stable/nn.html). For now let us use a sigmoid (`torch.nn.Sigmoid`).  \n","\n","<img src=\"https://miro.medium.com/v2/resize:fit:970/1*Xu7B5y9gp0iL5ooBj7LtWw.png\" width=\"450\"></br></br>\n","\n","\n","❗❗❗ Do not forget that a network must extend a `torch.nn.Module`.\n","\n","This means that when defining a neural network in PyTorch, you need to create a class that inherits from the  `torch.nn.Module ` class. This is because  `torch.nn.Module ` provides many useful features for defining neural networks, such as automatic parameter management, automatic calculation of gradients, and the ability to nest multiple modules within a network. So, when defining a network, it is important to have it inherit from  `torch.nn.Module ` to take advantage of these features."]},{"cell_type":"code","metadata":{"id":"b4MAGF_LKEQ5"},"source":["# Definizione della rete\n","class MLP(torch.nn.Module):\n","  def __init__(self, input_dim, hidden_dim, output_dim): #Prende in input le dimensioni dei layer di input, hidden e output\n","    super(MLP, self).__init__() # Questo chiama il costruttore della classe genitore (torch.nn.Module) per inizializzare la rete\n","\n","    self.input_to_hidden = torch.nn.Linear(input_dim, hidden_dim) # Questa riga crea un modulo lineare che rappresenta il layer da input a hidden\n","    self.hidden_to_output = torch.nn.Linear(hidden_dim, output_dim) # Questa riga crea un altro modulo lineare che rappresenta il layer da hidden a output\n","    self.activation = torch.nn.Sigmoid() # Qui viene creato un modulo per l'attivazione dei neuroni, che in questo caso è la funzione sigmoide\n","\n","  def forward(self, x): # Il metodo forward descrive come i dati passano attraverso la rete durante la fase di inoltro (forward pass)\n","    # Questo metodo è fondamentale per la fase di addestramento e di previsione del modello\n","    x = x.view(x.shape[0],-1) # Questa riga cambia la forma dei dati in ingresso x in modo che siano nella forma (batch_size, input_dim)\n","    # x.shape[0] è batch_size\n","    # -1 significa qualsiasi dimensione necessaria per mantenere invariato il numero totale di elementi originali.\n","    # (ENG) -1 refers to whatever size is necessary to maintain the original total number of elements.\n","\n","    # Forward input through the layers\n","    x = self.input_to_hidden(x) # Questa riga applica la trasformazione lineare dal layer di input al layer hidden.\n","    x = self.activation(x) # l'output del layer hidden viene passato attraverso la funzione di attivazione sigmoide.\n","    x = self.hidden_to_output(x) # l'output del layer hidden (dopo l'attivazione sigmoide) viene ulteriormente trasformato per produrre l'output finale del modello.\n","    # (ENG) the output of the hidden layer (after sigmoid activation) is further transformed to produce the final model output.\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TZMmfyFNKWGY"},"source":["## Loss/cost function\n","\n","Per addestrare la rete, abbiamo bisogno di un cost/loss function. Il compito è la classificazione con multiple classi, quindi cost/loss function appropriata potrebbe essere cross-entropy con softmax.\n","\n","Possiamo usare `torch.nn`, che contiene diverse cost/loss function, tra cui `torch.nn.CrossEntropyLoss`.\n","\n","Nota che `torch.nn.CrossEntropyLoss` contiene già l'attivazione softmax, quindi non è necessario applicare il softmax all'output della nostra rete.\n","\n","[ENG]\n","\n","For training the network, we need a loss function. The task is classification with multiple classes, thus a proper loss could be a cross-entropy with softmax.\n","\n","We can again use `torch.nn` which contains several losses, among which `torch.nn.CrossEntropyLoss`.\n","\n","Notice that `torch.nn.CrossEntropyLoss` already contains the softmax activation, thus we do not need to apply the softmax to the output of our network."]},{"cell_type":"code","metadata":{"id":"SFnlG_hCLpqJ"},"source":["def get_cost_function():\n","  cost_function = torch.nn.CrossEntropyLoss()\n","  return cost_function"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cJRtK9IcLsNB"},"source":["## Ottimizzatore (Optimizer)\n","\n","Ora dobbiamo trovare un modo per aggiornare i parametri della nostra rete.\n","Questo può essere fatto con  [`torch.optim`](https://pytorch.org/docs/stable/optim.html), che contiene una vasta varietà di ottimizzatori.\n","\n","[ENG]\n","\n","Now, we need to find a way to update the parameters of our network.\n","This can be done with [`torch.optim`](https://pytorch.org/docs/stable/optim.html) which contains a large variety of optimizers."]},{"cell_type":"code","metadata":{"id":"MeW_KX80Nwrr"},"source":["def get_optimizer(net, lr, wd, momentum): # net (un modello di rete neurale), lr (learning rate), wd (weigth decay), e momentum.\n","  optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum) # utilizzando l'algoritmo di discesa del gradiente stocastico (SGD)\n","                                                                                           # (ENG) Stochastic gradient descent\n","  return optimizer # restituisce l'oggetto ottimizzatore"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VpZ8snpiYRFK"},"source":["## Funzioni di Addestramento e Test\n","\n","Le funzioni di addestramento e test devono:\n","\n","1. Ciclare attraverso i dati (sfruttando il dataloader)\n","2. Eseguire il passaggio in avanti (forward pass) dei dati attraverso la rete\n","3. Calcolare *loss* per l'addestramento, *l'accuratezza* per il test o entrambi.\n","\n","Inoltre, la funzione di addestramento deve:\n","\n","1. Calcolare il gradiente con il passaggio all'indietro (`loss.backward()`)\n","2. Aggiornare *weights* (i pesi) utilizzando l'ottimizzatore (`optimizer.step()`)\n","3. Pulire il gradiente dei pesi in modo da non accumularlo (`optimizer.zero_grad()`)\n","\n","Definiamo:\n","\n","* **Iterazioni:** il numero di aggiornamenti del gradiente (cioè il numero di chiamate a `optimizer.step()`).\n","* **Epoca:** il numero di iterazioni che effettuiamo sull'intero dataset.\n","\n","\n","## Train and test functions\n","\n","[ENG]\n","\n","Training and test functions must:\n","\n","1.  Loop over the data (exploiting the dataloader)\n","2.  Forward pass the data through the network\n","3.  Compute the loss for train, the accuracy for test or both.\n","\n","Additionally, training function must:\n","\n","1.   Compute the gradient with the backward pass (`loss.backward()`)\n","2.   Update the weights by using the optimizer (`optimizer.step()`)\n","3.   Clean the gradient of the weights not to accumulating it (`optimizer.zero_grad()`)\n","\n","We define:\n","*   **Iterations:** the number of gradient updates (i.e. the number of calling the `optimizer.step()`).\n","*   **Epoch:** the number of iteratations we do over the whole dataset."]},{"cell_type":"markdown","source":["## Ricordiamo cosa è: Batch, Iterazione ed Epoca\n","\n","* Batch: è un sottoinsieme dell'intero dataset utilizzato durante *una singola iterazione* di addestramento.\n","* Iterazione: Un'iterazione è un singolo aggiornamento dei pesi del modello utilizzando *un batch* di dati.\n","* Epoca: Un'epoca rappresenta *un passaggio completo* attraverso l'intero dataset di addestramento.\n","\n","Esempio: Se abbiamo 1.000 esempi di addestramento e utilizziamo una dimensione di batch di 100, ci vorranno 10 iterazioni per elaborare un'epoca. Dopo la decima iterazione, avremo completato un'epoca, e il modello avrà analizzato tutti i 1.000 esempi di addestramento.\n","\n","[ENG]\n","\n","## RECAP: Batch, Iteration and Epoch\n","\n","* Batch: is a subset of the entire dataset that is used during *one iteration* of training.\n","* Iteration: An iteration is a single update of the model's weights using *one batch* of data.\n","* Epoch: An epoch is *a complete pass* through the entire training dataset.\n","\n","EXP: If you have 1,000 training examples and use a batch size of 100, it would take 10 iterations to process one epoch. After the 10th iteration, you will have completed one epoch, and the model will have seen all 1,000 training examples."],"metadata":{"id":"ZlFGV1ZUOiYs"}},{"cell_type":"code","metadata":{"id":"jHrTkmnWaZnp"},"source":["# Ciclo di Addestramento (Training Loop)\n","def train_one_epoch(net, data_loader, optimizer, cost_function, device='cuda'): # \"cuda\" per l'uso della GPU\n","  samples = 0. # per traccia del numero totale di campioni durante l'epoca\n","  cumulative_loss = 0. # per traccia la somma cumulativa delle perdite durante l'epoca\n","  cumulative_accuracy = 0. # per traccia la somma cumulativa delle accuratezze durante l'epoca\n","\n","\n","  net.train() # Alcuni layer possono comportarsi in modo diverso durante l'addestramento rispetto alla fase di test.\n","              # (ENG) Strictly needed if network contains layers which has different behaviours between train and test\n","\n","  for batch_idx, (inputs, targets) in enumerate(data_loader): # iterare sui batch di dati forniti da data_loader\n","    # Per ogni batch, i dati di input e i relativi target vengono caricati sulla GPU (Load data into GPU)\n","    inputs = inputs.to(device)\n","    targets = targets.to(device)\n","\n","    # Passaggio in Avanti (Forward pass)\n","    outputs = net(inputs)\n","\n","    # Applicazione della Funzione di Costo (Apply the loss)\n","    loss = cost_function(outputs,targets)\n","\n","    # Passaggio all'Indietro (Backward pass)\n","    loss.backward()\n","\n","    # Aggiornamento dei Parametri (Update parameters)\n","    optimizer.step()\n","\n","    # Pulizia dei Gradiente (Clean/reset the gradients)\n","    optimizer.zero_grad()\n","\n","    samples += inputs.shape[0] # traccia del numero totale di campioni che sono stati elaborati finora in questa epoca\n","    cumulative_loss += loss.item() # traccia della perdita totale accumulata durante l'epoca\n","\n","    # outputs.max(dim=1) restituisce due tensori: il primo contiene i valori massimi lungo la dimensione 1\n","    # che corrispondono alle probabilità predette per ciascuna classe\n","    # il secondo contiene gli indici dei massimi valori che rappresentano le classi predette.\n","    # Stiamo scartando il primo tensore (usando _) e conservando gli indici delle classi predette.\n","    _, predicted = outputs.max(dim=1) # (ENG) returns (maximum_value, index_of_maximum_value)\n","\n","    # stiamo confrontando gli indici delle classi predette (predicted) con gli indici delle classi target reali (targets) per il batch corrente.\n","    # La funzione predicted.eq(targets) restituisce un tensore di valori booleani che indicano se le predizioni sono corrette (True) o sbagliate (False).\n","    # Usando .sum(), contiamo quanti di questi valori booleani sono True (cioè quante predizioni sono corrette) e quindi convertiamo il risultato in un valore scalare con .item().\n","    # Questo valore scalare rappresenta il numero di predizioni corrette nel batch corrente.\n","    cumulative_accuracy += predicted.eq(targets).sum().item()\n","\n","  # la funzione restituisce due valori: la perdita media (cumulative_loss diviso per il numero totale di esempi)\n","  # e l'accuratezza media (cumulative_accuracy diviso per il numero totale di esempi, moltiplicato per 100 per ottenere la percentuale)\n","  return cumulative_loss/samples, cumulative_accuracy/samples*100"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Durante l'addestramento, è necessario testare il modello sul set di validazione per essere sicuri di migliorare le prestazioni.\n","\n","[ENG] While training, we need to test our model on the validaiton set to be sure we are improving the model performance."],"metadata":{"id":"ahCaPuG307cE"}},{"cell_type":"code","source":["# Fase di Validazione (Validation Step)\n","def validation_step(net, data_loader, cost_function, device='cuda'):\n","  samples = 0.\n","  cumulative_loss = 0.\n","  cumulative_accuracy = 0.\n","\n","  net.eval()  # Alcuni layer possono comportarsi in modo diverso durante l'addestramento rispetto alla fase di test.\n","              # (ENG) Strictly needed if network contains layers which has different behaviours between train and test\n","\n","  # torch.no_grad(): significa qualsiasi operazione che viene eseguita all'interno di questo contesto non influirà sui gradienti del modello.\n","  # In the following steps the gradient of the model will not be affected.\n","  with torch.no_grad():\n","    for batch_idx, (inputs, targets) in enumerate(data_loader): # iteriamo attraverso i batch di dati di convalida\n","      # Per ciascun batch, carichiamo i dati nella GPU (Load data into GPU)\n","      inputs = inputs.to(device)\n","      targets = targets.to(device)\n","\n","      # Passaggio in Avanti (Forward pass)\n","      outputs = net(inputs)\n","\n","      # Applicazione della Funzione di Costo (Apply the loss)\n","      loss = cost_function(outputs, targets)\n","\n","      samples+=inputs.shape[0]\n","      cumulative_loss += loss.item() # convertiamo il risultato (tensore) in un valore scalare con \".item()\"\n","      _, predicted = outputs.max(1)\n","      cumulative_accuracy += predicted.eq(targets).sum().item()\n","\n","  return cumulative_loss/samples, cumulative_accuracy/samples*100\n"],"metadata":{"id":"sNg3V8k81E7N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6QOcUCgwbers"},"source":["## Combiniamo il tutto (FUNZIONE PRINCIPALE)\n","\n","Infine, abbiamo bisogno di una funzione principale che:\n","\n","1) Inizializza tutto quello che abbiamo definito,\n","\n","2) Definisce gli iperparametri (*hyperparameters*),\n","\n","3) Esegue il ciclo su più epoche,\n","\n","4) Stampa i risultati.\n","\n","\n","[ENG]\n","## Let's combine everything (MAIN FUNCTION)\n","\n","Finally, we need a main function which\n","\n","1) Initializes everything,\n","\n","2) Defines hyperparameters,\n","\n","3) Loops over multiple epochs,\n","\n","4) Print the results."]},{"cell_type":"code","metadata":{"id":"x7eUGpq2b5YX"},"source":["import torchvision\n","import torch\n","\n","# Definire tutti gli iperparametri (batch_size..... epochs)\n","def main(batch_size=128, input_dim=28*28, hidden_dim=100, output_dim=10, device='cuda:0', learning_rate=0.01, weight_decay=0.000001, momentum=0.9, epochs=10):\n","\n","  # Ottiene i DataLoaders\n","  train_loader, val_loader, test_loader = get_data(batch_size)\n","\n","  # Inizializza la rete e la sposta sul dispositivo scelto (GPU).\n","  # (ENG) Initialize the network and moves it to GPU.\n","  net = MLP(input_dim, hidden_dim, output_dim).to(device)\n","\n","  # Istanzia l'ottimizzatore\n","  # (ENG) Instantiates the optimizer\n","  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n","\n","  # Crea la funzione di costo\n","  # (ENG) Creates the cost function\n","  cost_function = get_cost_function()\n","\n","  n_iterations = 0\n","  # Per ogni epoca, addestra la rete e poi calcola i risultati della valutazione.\n","  # (ENG) For each epoch, train the network and then compute evaluation results\n","  for e in range(epochs):\n","    train_loss, train_accuracy = train_one_epoch(net, train_loader, optimizer, cost_function)\n","    val_loss, val_accuracy = validation_step(net, val_loader, cost_function)\n","\n","    print('Epoch: {:d}'.format(e+1)) # Questo è un modo comune per tenere traccia dell'epoca corrente\n","    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n","    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n","    print('-----------------------------------------------------')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chiama la funzione principale\n","main()"],"metadata":{"id":"mzcQEworzhGW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ci si aspetta che ad ogni epoca la perdita durante l'addestramento diminuisca, mentre l'accuratezza durante la validazione aumenti.\n","\n","[ENG]\n","It is expected that at each epoch the loss during training decreases, while the accuracy during validation increases."],"metadata":{"id":"n2j7g1qdq6vO"}},{"cell_type":"markdown","source":["Tracciamo le curve di addestramento e di validazione per ciascuna epoca.\n","Per farlo, definiamo la funzione main2.\n","\n","[ENG] Let's plot the training and validation curves for each epoch. To do so we define main2."],"metadata":{"id":"fQLN5--nozHb"}},{"cell_type":"code","source":["import torchvision\n","import torch\n","from matplotlib import pyplot as plt\n","\n","# Definire tutti gli iperparametri (batch_size..... epochs)\n","def main2(batch_size=128, input_dim=28*28, hidden_dim=100, output_dim=10, device='cuda:0', learning_rate=0.01, weight_decay=0.000001, momentum=0.9, epochs=10):\n","\n","  trainingEpoch_loss = []\n","  validationEpoch_loss = []\n","  validationEpoch_accuracy = []\n","\n","  # Ottiene i DataLoaders\n","  train_loader, val_loader, test_loader = get_data(batch_size)\n","\n","  # Inizializza la rete e la sposta sul dispositivo scelto (GPU).\n","  # (ENG) Initialize the network and moves it to GPU.\n","  net = MLP(input_dim, hidden_dim, output_dim).to(device)\n","\n","  # Istanzia l'ottimizzatore\n","  # (ENG) Instantiates the optimizer\n","  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n","\n","  # Crea la funzione di costo\n","  # (ENG) Creates the cost function\n","  cost_function = get_cost_function()\n","\n","  n_iterations = 0\n","  # Per ogni epoca, addestra la rete e poi calcola i risultati della valutazione.\n","  # (ENG) For each epoch, train the network and then compute evaluation results\n","  for e in range(epochs):\n","    train_loss, train_accuracy = train_one_epoch(net, train_loader, optimizer, cost_function)\n","    val_loss, val_accuracy = validation_step(net, val_loader, cost_function)\n","\n","    trainingEpoch_loss.append(train_loss) # Raccogla la perdita di allenamento\n","    validationEpoch_loss.append(val_loss) # Raccogla la perdita della convalida\n","    # validationEpoch_accuracy.append(val_accuracy) # Raccogla l'accuratezza della convalida\n","\n","\n","  plt.plot(trainingEpoch_loss, label='training loss')\n","  plt.plot(validationEpoch_loss,label='validation loss')\n","  # plt.plot(validationEpoch_accuracy,label='validation accuracy')\n","  plt.title('Training and Validation Curves')\n","  plt.xlabel('Epoch')\n","  plt.legend()\n","  plt.show\n"],"metadata":{"id":"D53A3ApNsI9q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["main2()"],"metadata":{"id":"lLgO4RKytTGZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Esercizio 1\n","\n","**1)** Esegui lo stesso codice quando:\n","\n","* batch_size=128\n","* input_dim=28*28\n","* hidden_dim=100\n","* output_dim=10\n","* device='cuda:0'\n","* learning_rate=0.01\n","* weight_decay=0.000001\n","* momentum=0.9\n","* epoche=10\n","* con l'ottimizzatore Adam: `optimizer = torch.optim.Adam(net.parameters())`.\n","\n","**2)** Utilizza `torch.save(net.state_dict(), \"model.ckpt\")`.\n","\n","**ATTENZIONE:** `torch.save(net.state_dict(), \"model.ckpt\")` è una riga di codice che salva i pesi (o i parametri) di una rete neurale PyTorch nel file `model.ckpt`. Questo permette di conservare i parametri addestrati della rete in modo da poterli riutilizzare in futuro, ad esempio per fare previsioni o per continuare l'addestramento. Quando si desidera utilizzare la rete in un'altra sessione o in un altro momento, è possibile caricare questi pesi utilizzando la funzione `load_state_dict()` e inizializzare una rete con i pesi precedentemente addestrati. Questo è particolarmente utile quando si lavora su progetti di deep learning che richiedono molto tempo per l'addestramento e si desidera salvare i progressi o condividere modelli con altri.\n","\n","**3)** Carica i pesi salvati e addestra ed esegui la valutazione per 5 epoche con gli stessi iperparametri.\n","\n","Per eseguire ciò:\n","\n","a) Il modello deve essere creato.\n","\n","b) Il modello deve essere spostato sul dispositivo specificato ('cuda:0') utilizzando `net = net.to(device)`.\n","\n","c) I pesi sono caricati nel modello utilizzando `net.load_state_dict`.\n","\n","d) L'ottimizzatore è creato con il tasso di apprendimento (learning rate) e weigth decay.\n","\n","e) Sia il passaggio di addestramento che quello di validazione passano il dispositivo come argomento.\n","\n","%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n","\n","[ENG]\n","\n","**1)** Run the same code when:\n","\n","* batch_size=128\n","* input_dim=28*28\n","* hidden_dim=100\n","* output_dim=10\n","* device='cuda:0'\n","* learning_rate=0.01\n","* weight_decay=0.000001\n","* momentum=0.9\n","* epochs=10\n","* with Adam optimizer optimizer = `torch.optim.Adam(net.parameters())`.\n","\n","**2)** Use `torch.save(net.state_dict(), \"model.ckpt\")`.\n","\n","\n","**IMPORTANT:** `torch.save(net.state_dict(), \"model.ckpt\")` is a line of code that saves the weights (or parameters) of a PyTorch neural network to the `model.ckpt` file. This allows you to keep the trained parameters of the network so you can reuse them in the future, for example to make predictions or to continue training. When you want to use the network in another session or at another time, you can load these weights using the `load_state_dict()` function and initialize a network with the previously trained weights. This is especially useful when you're working on deep learning projects that require a lot of training time and you want to save your progress or share models with others.\n","\n","**3)** Load the saved weigths and train and evaluate 5 epochs with the same hyperparameters.\n","\n","To perform this:\n","\n","a) The model should be created\n","\n","b) The model should be moved to the specified device ('cuda:0') using `net = net.to(device)`.\n","\n","c) The weights are loaded into the model using `net.load_state_dict`.\n","\n","d) The optimizer is created with the specified learning rate and weight decay.\n","\n","e) Both the training and validation steps pass the device as an argument."],"metadata":{"id":"owbv88O8ohWT"}},{"cell_type":"markdown","source":["# Esercizio 2\n","\n","## Eseguire una task di classificazione con DATI TABULARI (dataset del Titanic) usando MLP.\n","\n","* Scarica il dataset del Titanic dall'URL fornito: ``` https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv ```\n","* Si tratta di un compito di classificazione con due classi in cui le etichette sono memorizzate in 'Survived'.\n","* Come metodo di classificazione, utilizzerai MLP.\n","* Utilizzerai tutte le caratteristiche fornite, che includono: Pclass (categorical), Name, Sex (Categorical), Age,  Siblings/Spouses Aboard, Parents/Children Aboard and Fare.\n","* Rimuovi le colonne non necessarie: eliminiamo solo \"Name\" anche se molte altre colonne potrebbero essere irrilevanti.\n","* Applica la codifica one-hot per le variabili categoriali.\n","* Elimina gli esempi con valori mancanti (NA).``` dropna() ```\n","* Dividi i dati in modo che il set di addestramento contenga l'80% e il set di test il 20% del numero totale di dati. Non è necessario applicare la cross-validation. Dato che si tratta di un dataset molto piccolo, non è necessario utilizzare batch.\n","* Standardizza le caratteristiche (la standardizzazione è un tipo di normalizzazione).\n","```\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","```\n","* Definisci un modello MLP personalizzato in modo che abbia uno strato (livello) di input, due strati (livelli) nascosto e uno strato di output. Usa la funzione di attivazione ReLU in modo appropriato. ```nn.ReLU()```\n","\n","* Scrivi la funzione principale (main) quando:\n","```\n","input_dim = X_train.shape[1]\n","hidden_dim1 = 64\n","hidden_dim2 = 32\n","output_dim = 2  # 2 classes: Survived or not\n","```\n","* Definisci una funzione di costo che ritieni adatta per questo compito.\n","* Scegli un ottimizzatore con un \"learning rate\" che ritieni appropriato.\n","* Nota che i dati sono TABULAR, quindi è necessario convertirli in tensori PyTorch per il processo.\n","```\n","X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n","```\n","* Addestra il classificatore per 100 epoche.\n","* Testa il modello addestrato con i dati di test e stampa l'accuratezza finale.\n","\n","\n","\n","\n","\n","[ENG]\n","\n","## Performing a classification task with TABULAR DATA (titanic dataset) using MLP.\n","\n","* Download the titanic dataset from the supplied URL: ``` https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv ```\n","* This is a classification task with two classes in which the labels are stored in “Survived”.\n","* As the classification method you will be using MLP.\n","* You will be using all features supplied which are: Pclass (categorical), Name, Sex (Categorical), Age,  Siblings/Spouses Aboard, Parents/Children Aboard and Fare.\n","* Drop unnecessary columns: we drop only “Name” even though many other column can be irrelevant.\n","* Apply one-hot encoding for the categorical variables.\n","* Drop examples with NA values. ``` dropna() ```\n","* Split the data such that train has 80% and test is 20% of the total number of data. You do not need to apply cross validation. Since this is a very small dataset, we do not need to use batch.\n","* Standardize features (standardization is a kind of normalization).\n","```\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","```\n","- Define a custom MLP model such that it has an input layer, a hidden layer and an output layer. Use ReLU in an appropriate way. ```nn.ReLU()```\n","- Write the main function when\n","```\n","input_dim = X_train.shape[1]\n","hidden_dim1 = 64\n","hidden_dim2 = 32\n","output_dim = 2  # 2 classes: Survived or not\n","```\n","-\tDefine a cost function that you believe it is suitable for this task.\n","- Pick an optimizer with a learning rate you believe appropriate.\n","- Note that the data is TABULAR so you need to convert it to PyTorch tensors to process.\n","```\n","X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n","```\n","-\tTrain the classifier for 100 epochs.\n","-\tTest the trained model with testing data and print out the final accuracy."],"metadata":{"id":"JKqET6Ih3NZN"}}]}