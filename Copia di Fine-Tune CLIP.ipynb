{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1wu3n19owBFAtpLWuzt9rY-QV43aIs5EV","timestamp":1702393908597}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Blkz792TNNtF","executionInfo":{"status":"ok","timestamp":1702840315608,"user_tz":-60,"elapsed":64683,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}},"outputId":"99d8ef24-7426-4840-b493-97fa68707d96"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/53.4 kB\u001b[0m \u001b[31m505.4 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m51.2/53.4 kB\u001b[0m \u001b[31m687.8 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m568.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hMounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 278M/278M [00:03<00:00, 80.1MiB/s]\n"]}],"source":["#@title CLIP preprocess and Imports\n","!pip -q install ftfy regex tqdm\n","!pip -q install git+https://github.com/openai/CLIP.git\n","#!pip -q install tensorflow_addons torchmetrics\n","import os\n","import numpy as np\n","import h5py\n","import torch\n","import clip\n","import cv2\n","from PIL import Image\n","from IPython.display import Image as ImagePy, display\n","import matplotlib.pyplot as plt\n","import torchvision.transforms as transforms\n","from tqdm import tqdm\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","clip_model, preprocess = clip.load(\"RN101\")\n","dataset_path = \"/content/data.h5\""]},{"cell_type":"code","source":["!cp \"/content/drive/MyDrive/TESI/CODICE/Columbia Dataset/Frames_labels_dataset/frames_dataset.h5\" \"/content/data.h5\""],"metadata":{"id":"vZmd-e-TOoxJ","executionInfo":{"status":"ok","timestamp":1702840575286,"user_tz":-60,"elapsed":259148,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["people_list = [#\"bell\",\n","               \"sick\",\n","               \"long\",\n","               \"bollinger\",\n","               \"lieberman\",\n","               ]"],"metadata":{"id":"QL8W9aYkNTi7","executionInfo":{"status":"ok","timestamp":1702840575286,"user_tz":-60,"elapsed":3,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#@title Get Indices\n","from sklearn.model_selection import train_test_split\n","train_val_indices = {}\n","with h5py.File(dataset_path, 'r') as f:\n","    for p in people_list:\n","      person = f[p]\n","      indices = person['frames'].attrs.get(\"data_length\")\n","      train_indices, val_indices = train_test_split(np.arange(indices), test_size=0.2, random_state=42)\n","      train_val_indices[person.name.lstrip('/')] = (train_indices, val_indices)"],"metadata":{"id":"hEyeuejpGA4n","executionInfo":{"status":"ok","timestamp":1702840590184,"user_tz":-60,"elapsed":1628,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#@title Check Chosen People Data\n","print(\"----------------------- TRAIN DATA ---- VAL DATA --- TOTAL DATA\")\n","with h5py.File(dataset_path, 'r') as f:\n","    tot_train = 0\n","    tot_val = 0\n","    for p in people_list:\n","      person = f[p]\n","      total = person['frames'].attrs.get('data_length')\n","\n","      train_size = len(train_val_indices[p][0])\n","      val_size = len(train_val_indices[p][1])\n","\n","      tot_train += train_size\n","      tot_val += val_size\n","      print(f'Group {p}:      \\t{train_size} \\t\\t{val_size} \\t\\t{total}')\n","\n","    print(f\"TOTAL DATA------------- {tot_train} --------- {tot_val} ---------- {tot_train+tot_val}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sRNcD6XuNTkl","executionInfo":{"status":"ok","timestamp":1702840692010,"user_tz":-60,"elapsed":223,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}},"outputId":"d6ff4b6e-62a7-4d71-af04-8e7f3dbd2e8a","cellView":"form"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------- TRAIN DATA ---- VAL DATA --- TOTAL DATA\n","Group sick:      \t30820 \t\t7706 \t\t38526\n","Group long:      \t23512 \t\t5879 \t\t29391\n","Group bollinger:      \t12064 \t\t3016 \t\t15080\n","Group lieberman:      \t13120 \t\t3280 \t\t16400\n","TOTAL DATA------------- 79516 --------- 19881 ---------- 99397\n"]}]},{"cell_type":"code","source":["#@title Get Class Weights\n","\n","class_counts = {}\n","\n","# Open the HDF5 file\n","with h5py.File(dataset_path, 'r') as f:\n","    # Iterate over each group (person) in the file\n","    for p in people_list:\n","        person = f[p]\n","        labels = []\n","        for index in train_val_indices[p][0]:\n","          labels.append(person['labels'][index])\n","        # Count occurrences of each class in the labels\n","        for label in labels:\n","            class_counts[label] = class_counts.get(label, 0) + 1\n","\n","# Calculate total number of training samples\n","total_train_samples = sum(class_counts.values())\n","\n","# Compute class weights as the inverse of class frequencies\n","class_weights = {cls: total_train_samples / count for cls, count in class_counts.items()}\n","\n","# Normalize the weights so that they sum up to 1 (optional)\n","total_weight = sum(class_weights.values())\n","class_weights_normalized = {cls: weight / total_weight for cls, weight in class_weights.items()}\n","print(class_weights_normalized)\n","# Convert class weights to a tensor\n","class_weights_tensor = torch.tensor(list(class_weights_normalized.values()), dtype=torch.float)\n","positive_class_weight_tensor = class_weights_tensor[1].clone().detach().cuda()"],"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"oma2giKZGNuv","executionInfo":{"status":"ok","timestamp":1702840705524,"user_tz":-60,"elapsed":10706,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}},"outputId":"e004acca-54c5-4599-8aa8-819ff40facb0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 0.37242819055284476, 1: 0.6275718094471554}\n"]}]},{"cell_type":"code","source":["#@title Data Generator\n","class FineTuneDataGenerator:\n","    def __init__(self, data_directory, batch_size, preprocess_f, speakers, mode):\n","        self.data_directory = data_directory\n","        self.batch_size = batch_size\n","        self.speakers = speakers\n","        self.mode = mode  # 'train' or 'validation'\n","        self.preprocess = preprocess_f\n","\n","    def __iter__(self):\n","        with h5py.File(self.data_directory, 'r') as h5_file:\n","            for speaker in self.speakers:\n","                frames_dataset = h5_file[speaker]['frames']\n","                labels_dataset = h5_file[speaker]['labels']\n","                data_length = frames_dataset.attrs['data_length']\n","                train_split_index = frames_dataset.attrs['train_split_index']\n","\n","                # Determine the indices for training or validation split\n","                if self.mode == 'train':\n","                    indices = np.arange(train_split_index)\n","                else:\n","                    indices = np.arange(train_split_index, data_length)\n","\n","                np.random.shuffle(indices)\n","\n","                # Yield batches\n","                for i in range(0, len(indices), self.batch_size):\n","                    batch_indices = indices[i:i + self.batch_size]\n","                    batch_indices = np.sort(batch_indices)\n","\n","                    frames = []\n","                    for index in batch_indices:\n","\n","                      f = Image.fromarray(frames_dataset[index])\n","                      f = f.convert('RGB') if f.mode != 'RGB' else f\n","                      f = preprocess(f)\n","                      frames.append(f)\n","\n","                    labels = labels_dataset[batch_indices]\n","                    yield torch.stack(frames), np.array(labels)"],"metadata":{"cellView":"form","id":"_dDHMEQ9NThV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Data Loader con miglior shuffle tra Speakers\n","class FineTuneDataGenerator:\n","    def __init__(self, data_directory, batch_size, preprocess_f, indices, mode):\n","        self.data_directory = data_directory\n","        self.batch_size = batch_size\n","        self.preprocess = preprocess_f\n","        self.groups = list(indices.keys())\n","        self.mode = mode  # 'train' or 'validation'\n","        self.indices_dict = indices\n","\n","        self.h5_file = h5py.File(self.data_directory, 'r')\n","        group_data = []\n","        for group in self.groups:\n","              frames_dataset = self.h5_file[group]['frames']\n","              labels_dataset = self.h5_file[group]['labels']\n","\n","              if self.mode == 'train':\n","                group_data.append((frames_dataset, labels_dataset, self.indices_dict[group][0]))\n","              else:\n","                group_data.append((frames_dataset, labels_dataset, self.indices_dict[group][1]))\n","\n","        self.group_data = group_data\n","\n","    def close_file(self):\n","      self.h5_file.close()\n","\n","    def shuffle_replenish_data(self):\n","        # print(\"\\nBefore Shuffle:\")\n","        # for frames_dataset, labels_dataset, indices in self.group_data:\n","        #   print(f\"\\t lunghezza indici:{len(indices)}\")\n","\n","        self.group_data.clear() # resetta la lista di gruppi USATI\n","\n","        for group in self.groups:\n","              frames_dataset = self.h5_file[group]['frames']\n","              labels_dataset = self.h5_file[group]['labels']\n","\n","              if self.mode == 'train':\n","                shuffled_indices = np.random.permutation(self.indices_dict[group][0])\n","                self.group_data.append((frames_dataset, labels_dataset, shuffled_indices))\n","              else:\n","                shuffled_indices = np.random.permutation(self.indices_dict[group][1])\n","                self.group_data.append((frames_dataset, labels_dataset, shuffled_indices))\n","\n","        # print(\"\\nAfter shuffle: \")\n","        # for frames_dataset, labels_dataset, indices in self.group_data:\n","        #   print(f\"\\t lunghezza indici:{len(indices)}\")\n","\n","\n","    def __len__(self):\n","        length = 0\n","        for group in self.group_data:\n","          length += len(group[2])\n","\n","        return length // self.batch_size\n","\n","    def __iter__(self):\n","\n","            batch_frames = []\n","            batch_labels = []\n","\n","            # Calculate the number of data points to take from each group per batch\n","            data_per_group = self.batch_size // len(self.groups)\n","\n","            if self.mode == 'train':\n","              # Interleave data from different groups\n","              while any(len(data[2]) for data in self.group_data):\n","                  for _ in range(data_per_group):\n","                      for i, (frames_dataset, labels_dataset, indices) in enumerate(self.group_data):\n","                          if len(indices):\n","                              index = indices[0]\n","                              self.group_data[i] = (frames_dataset, labels_dataset, indices[1:])  # Update the indices in group_data\n","                              frame = frames_dataset[index]\n","                              label = labels_dataset[index]\n","                              f = Image.fromarray(frame)\n","                              f = f.convert('RGB') if f.mode != 'RGB' else f\n","                              batch_frames.append(self.preprocess(f))\n","                              #batch_frames.append(torch.from_numpy(frame))\n","                              batch_labels.append(label)\n","\n","                              # Check if the batch is complete\n","                              if len(batch_frames) == self.batch_size:\n","                                  # Yield the batch and reset the containers\n","                                  yield torch.stack(batch_frames), torch.tensor(batch_labels)\n","\n","                                  batch_frames = []\n","                                  batch_labels = []\n","\n","              # Yield any remaining data as the last batch\n","              if batch_frames:\n","                  yield torch.stack(batch_frames), torch.tensor(batch_labels)\n","\n","            else: # 'validation', no interleaving between groups\n","              for frames_dataset, labels_dataset, indices in self.group_data:\n","                  for index in indices:\n","                      frame = frames_dataset[index]\n","                      label = labels_dataset[index]\n","                      f = Image.fromarray(frame)\n","                      f = f.convert('RGB') if f.mode != 'RGB' else f\n","                      batch_frames.append(self.preprocess(f))\n","                      batch_labels.append(label)\n","\n","                      if len(batch_frames) == self.batch_size:\n","                          # Yield the batch and reset the containers\n","                          yield torch.stack(batch_frames), torch.tensor(batch_labels)\n","                          batch_frames = []\n","                          batch_labels = []\n","\n","              # Yield any remaining data as the last batch\n","              if batch_frames:\n","                  yield torch.stack(batch_frames), torch.tensor(batch_labels)\n"],"metadata":{"id":"f3iRH8PMNTfx","cellView":"form","executionInfo":{"status":"ok","timestamp":1702840705524,"user_tz":-60,"elapsed":2,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#@title Custom Dataset\n","\n","from torch.utils.data import Dataset\n","\n","def worker_init_fn(worker_id):\n","    global h5_file\n","    h5_file = h5py.File(dataset_path, 'r')\n","\n","class FineTuneDataset(Dataset):\n","    def __init__(self, data_directory, preprocess_f, speakers, mode):\n","        self.data_directory = data_directory\n","        self.preprocess = preprocess_f\n","        self.groups = speakers\n","        self.mode = mode  # 'train' or 'validation'\n","        self.h5_file = h5py.File(self.data_directory, 'r')  # Open the HDF5 file\n","        self.length = 0  # Initialize the length of the dataset\n","        self.indices = []  # Initialize the list of indices for each group\n","        for group in self.groups:\n","            frames_dataset = self.h5_file[group]['frames']\n","            labels_dataset = self.h5_file[group]['labels']\n","            data_length = frames_dataset.attrs['data_length']\n","            train_split_index = frames_dataset.attrs['train_split_index']\n","\n","            # Determine the indices for training or validation split\n","            if self.mode == 'train':\n","                indices = np.arange(train_split_index)\n","                self.length += train_split_index\n","            else:\n","                indices = np.arange(train_split_index, data_length)\n","                self.length += (data_length - train_split_index)\n","\n","            # Shuffle indices within each group\n","            np.random.shuffle(indices)\n","            self.indices.append(indices)\n","\n","    def __len__(self):\n","        return self.length\n","\n","    def __getitem__(self, idx):\n","        # Find the group and the index within the group for the given idx\n","        group_idx = 0\n","        group_length = len(self.indices[0])\n","        while idx >= group_length:\n","            idx -= group_length\n","            group_idx += 1\n","            group_length = len(self.indices[group_idx])\n","\n","        # Get the frame and label datasets for the group\n","        group = self.groups[group_idx]\n","        frames_dataset = self.h5_file[group]['frames']\n","        labels_dataset = self.h5_file[group]['labels']\n","\n","        # Get the frame and label for the index within the group\n","        index = self.indices[group_idx][idx]\n","        frame = frames_dataset[index]\n","        label = labels_dataset[index]\n","\n","        # Convert the frame to a PIL image and apply the preprocessing function\n","        f = Image.fromarray(frame)\n","        f = f.convert('RGB') if f.mode != 'RGB' else f\n","        frame = self.preprocess(f)\n","\n","        # Return the frame and label as tensors\n","        return frame, label\n"],"metadata":{"cellView":"form","id":"zmcZVVmm9m9R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Custom Dataset con Sampler\n","from torch.utils.data import Dataset, DataLoader, Sampler\n","class FineTuneDataset2(Dataset):\n","    def __init__(self, data_directory, preprocess_f, speakers, mode):\n","        self.data_directory = data_directory\n","        self.preprocess = preprocess_f\n","        self.groups = speakers\n","        self.mode = mode  # 'train' or 'validation'\n","        self.h5_file = h5py.File(self.data_directory, 'r')  # Open the HDF5 file\n","        self.indices = []  # Initialize the list of indices for each group\n","        self.group_lengths = []  # Initialize the list of lengths for each group\n","        for group in self.groups:\n","            frames_dataset = self.h5_file[group]['frames']\n","            labels_dataset = self.h5_file[group]['labels']\n","            data_length = frames_dataset.attrs['data_length']\n","            train_split_index = frames_dataset.attrs['train_split_index']\n","\n","            # Determine the indices for training or validation split\n","            if self.mode == 'train':\n","                indices = np.arange(train_split_index)\n","            else:\n","                indices = np.arange(train_split_index, data_length)\n","\n","            # Shuffle indices within each group\n","            np.random.shuffle(indices)\n","            self.indices.append(indices)\n","            self.group_lengths.append(len(indices))\n","\n","    def __len__(self):\n","        return sum(self.group_lengths)\n","\n","    def __getitem__(self, idx):\n","        # Find the group and the index within the group for the given idx\n","        group_idx = 0\n","        while idx >= self.group_lengths[group_idx]:\n","            idx -= self.group_lengths[group_idx]\n","            group_idx += 1\n","\n","        # Get the frame and label datasets for the group\n","        group = self.groups[group_idx]\n","        frames_dataset = self.h5_file[group]['frames']\n","        labels_dataset = self.h5_file[group]['labels']\n","\n","        # Get the frame and label for the index within the group\n","        index = self.indices[group_idx][idx]\n","        frame = frames_dataset[index]\n","        label = labels_dataset[index]\n","\n","        # Convert the frame to a PIL image and apply the preprocessing function\n","        f = Image.fromarray(frame)\n","        f = f.convert('RGB') if f.mode != 'RGB' else f\n","        frame = self.preprocess(f)\n","\n","        # Return the frame and label as tensors\n","        return frame, label\n","\n","class InterleavedBatchSampler(Sampler):\n","    def __init__(self, dataset, batch_size):\n","        self.dataset = dataset\n","        self.batch_size = batch_size\n","\n","    def __iter__(self):\n","        batch = []\n","        group_indices = [list(indices) for indices in self.dataset.indices]\n","        group_order = list(range(len(self.dataset.groups)))  # Order in which groups are accessed\n","\n","        while any(group_indices):\n","            for group_idx in group_order:\n","                indices = group_indices[group_idx]\n","                if indices:\n","                    batch.append(indices.pop(0))\n","                    if len(batch) == self.batch_size:\n","                        yield batch\n","                        batch = []\n","        if batch:\n","            yield batch\n","\n","    def __len__(self):\n","        return len(self.dataset)"],"metadata":{"cellView":"form","id":"DJF-pSWJE5lm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title TEST GENERATOR\n","# generator semplice:             0.28 sec\n","# generator con miglior shuffle:  0.39 sec\n","import time\n","\n","data_generator = FineTuneDataGenerator(dataset_path, 64, preprocess, ['bell', 'sick', 'long', 'bollinger'], 'train')\n","\n","# Generate one batch of data\n","try:\n","    start = time.time()\n","    frames, labels = next(iter(data_generator))\n","    end = time.time()\n","    labels = labels.unsqueeze(1)\n","    print(f\"Batch generated successfully in time: {end-start}\")\n","    print(\"Frames shape:\", frames.shape)\n","    print(\"Labels shape:\", labels.shape)\n","except StopIteration:\n","    print(\"No more data to generate.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8aEbv1BVNTeJ","executionInfo":{"status":"ok","timestamp":1702486996402,"user_tz":-60,"elapsed":446,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}},"outputId":"d40562ea-aa8c-4722-c8ef-693a5f8ab324","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Batch generated successfully in time: 0.2429492473602295\n","Frames shape: torch.Size([64, 3, 224, 224])\n","Labels shape: torch.Size([64, 1])\n"]}]},{"cell_type":"code","source":["import cv2\n","\n","unnormalize = transforms.Normalize((-0.48145466/0.26862954, -0.4578275/0.26130258, -0.40821073/0.27577711), (1/0.26862954, 1/0.26130258, 1/0.27577711))\n","img = transforms.ToPILImage()(unnormalize(frames[4])).convert(\"RGB\")\n","img = np.array(img)\n","cv2.imwrite('test.jpg', img)\n","print(labels[2])\n","display(ImagePy('test.jpg'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":258},"id":"AglEFX5iNbL7","executionInfo":{"status":"ok","timestamp":1702487014292,"user_tz":-60,"elapsed":3,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}},"outputId":"13aef558-f42a-41cc-9c4d-17c724e5642f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0], dtype=torch.uint8)\n"]},{"output_type":"display_data","data":{"image/jpeg":"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\n","text/plain":["<IPython.core.display.Image object>"]},"metadata":{}}]},{"cell_type":"code","source":["#@title Custom Clip Model\n","class CustomCLIP(torch.nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    model = clip_model\n","\n","    # modal.visual: prendere il codificatore visuale di CLIP (visual encoder of CLIP)\n","    # float: l'encoder è convertito a 32 bit poiché il CLIP predefinito è a 16 bit\n","    self.encoder = model.visual.float()\n","\n","    # aggiunge uno strato lineare (add a single linear layer)\n","    self.classifier = torch.nn.Linear(512, 1)\n","\n","  # Il simbolo -> viene utilizzato per specificare il tipo di valore restituito dalla funzione\n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","    x = self.encoder(x)\n","    x = self.classifier(x)\n","\n","    return x\n","\n","net = CustomCLIP().to(device)\n","\n","trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n","print(f'Number of trainable parameters: {trainable_params}')"],"metadata":{"id":"n_yi65-jNbKO","executionInfo":{"status":"ok","timestamp":1702840705524,"user_tz":-60,"elapsed":1,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"64326f14-4edf-4da1-919e-89d1269d8f62"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of trainable parameters: 56260449\n"]}]},{"cell_type":"code","source":["net.load_state_dict(torch.load(\"/content/drive/MyDrive/TESI/CODICE/CLIP_Models/ResNet_5_epochs_no_bell.pth\"))"],"metadata":{"id":"eSzDvj05rvZQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702840816356,"user_tz":-60,"elapsed":2679,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}},"outputId":"fd1cc109-0c9f-496c-e30f-bb6266962014"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["#@title Training & Test Steps\n","from tqdm import tqdm\n","def training_step(net, data_loader, optimizer, cost_function, device='cuda'):\n","    samples = 0.0\n","    cumulative_loss = 0.0\n","    cumulative_accuracy = 0.0\n","\n","    # Set the network to training mode\n","    net.train()\n","\n","    # Iterate over the training set\n","    for batch_idx, (frames, labels) in enumerate(tqdm(data_loader, desc=\"Train Progress\")):\n","        frames = frames.to(device)\n","        labels = labels.float().unsqueeze(1).to(device)\n","        # Forward pass\n","        outputs = net(frames)\n","\n","        # Loss computation\n","        loss = cost_function(outputs, labels)\n","\n","        optimizer.zero_grad()\n","\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        samples += frames.shape[0]\n","        cumulative_loss += loss.item()\n","        #_, predicted = outputs.max(1)\n","\n","        probabilities = torch.sigmoid(outputs)\n","        predictions = (probabilities > 0.5).float()\n","\n","        # Compute training accuracy\n","        cumulative_accuracy += predictions.eq(labels).sum().item()\n","\n","    if samples==0: print(\"ZERO SAMPLES GENERATED in train!!!\")\n","    return cumulative_loss / samples, cumulative_accuracy / samples\n","\n","def test_step(net, data_loader, cost_function, device='cuda'):\n","    samples = 0.0\n","    cumulative_loss = 0.0\n","    cumulative_accuracy = 0.0\n","\n","    # Set the network to evaluation mode\n","    net.eval()\n","\n","    with torch.no_grad():\n","        # Iterate over the test set\n","        for batch_idx, (frames, labels) in enumerate(tqdm(data_loader, desc=\"Eval Progress\")):\n","            frames = frames.to(device)\n","            labels = labels.float().unsqueeze(1).to(device)\n","\n","            # Forward pass\n","            outputs = net(frames)\n","\n","            # Loss computation\n","            loss = cost_function(outputs, labels)\n","\n","            samples += frames.shape[0]\n","            cumulative_loss += loss.item()\n","            #_, predicted = outputs.max(1)\n","\n","            probabilities = torch.sigmoid(outputs)\n","            predictions = (probabilities > 0.5).float()\n","\n","            # Compute test accuracy\n","            cumulative_accuracy += predictions.eq(labels).sum().item()\n","\n","    if samples==0: print(\"ZERO SAMPLES GENERATED in val!!!\")\n","    return cumulative_loss / samples, cumulative_accuracy / samples\n"],"metadata":{"id":"6wCV0EoVNbIm","cellView":"form","executionInfo":{"status":"ok","timestamp":1702840819512,"user_tz":-60,"elapsed":207,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["#@title TEST 1 TRAINING EPOCH\n","\n","mode = \"train\"\n","data_loader = FineTuneDataGenerator(dataset_path, 64, preprocess, people_list, mode)\n","# custom_dataset = FineTuneDataset(dataset_path, preprocess, people_list, 'train')\n","# data_loader = torch.utils.data.DataLoader(custom_dataset, batch_size=64, shuffle=True,\n","#                                           num_workers=2, worker_init_fn=worker_init_fn, pin_memory=True)\n","\n","optimizer = torch.optim.Adam(net.parameters(), lr=args['learning_rate'])\n","cost_function = torch.nn.BCEWithLogitsLoss(pos_weight=positive_class_weight_tensor)\n","\n","loss, accuracy = training_step(net, data_loader, optimizer, cost_function, device='cuda')\n","\n","# Print the results\n","print(f\"Average loss: {loss:.4f}, Average accuracy: {accuracy:.2f}%\")"],"metadata":{"id":"Ce_frlhmVWV_","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = {\n","      'batch_size':32,\n","      'device':'cuda:0',\n","      'learning_rate':0.001,\n","      'weight_decay':0.0005, # L2 Regularization, aumento se overfitta\n","      'momentum':0.9,\n","      'epochs':5,\n","      'people_list':people_list,\n","      'class_weight':positive_class_weight_tensor,\n","      'indices': train_val_indices\n","    }\n","\n","# se vedo che la LOSS cambia troppo poco --> aumento Learning Rate (e viceversa se ondeggia troppo)"],"metadata":{"id":"M7D8PDCheXS_","executionInfo":{"status":"ok","timestamp":1702840832599,"user_tz":-60,"elapsed":202,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["!pip -q install wandb\n","import wandb\n","!wandb login"],"metadata":{"id":"F5Xs3kdtd-oo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702840886470,"user_tz":-60,"elapsed":51001,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}},"outputId":"6578a1a3-1b83-4d6c-ac6b-1bcc5e0a485b"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}]},{"cell_type":"code","source":["wandb.init(\n","    # set the wandb project where this run will be logged\n","    project=\"CLIP ResNet101 Finetuning\",\n","    name=\"2 - no Bell bs=32\",\n","\n","    # track hyperparameters and run metadata\n","    config={\n","    \"learning_rate\": args['learning_rate'],\n","    \"epochs\": args['epochs'],\n","    \"batch_size\":args['batch_size'],\n","    'weight_decay':args[\"weight_decay\"]\n","\n","    }\n",")"],"metadata":{"id":"hh-nc_TPeEoc","colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"status":"ok","timestamp":1702840946083,"user_tz":-60,"elapsed":3593,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}},"outputId":"a5a78675-7497-4b8b-f84f-42c6ccc01fe7"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ma-appiani2\u001b[0m (\u001b[33mandrea-unibg\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20231217_192224-ow47emw6</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/andrea-unibg/CLIP%20ResNet101%20Finetuning/runs/ow47emw6' target=\"_blank\">2 - no Bell bs=32</a></strong> to <a href='https://wandb.ai/andrea-unibg/CLIP%20ResNet101%20Finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/andrea-unibg/CLIP%20ResNet101%20Finetuning' target=\"_blank\">https://wandb.ai/andrea-unibg/CLIP%20ResNet101%20Finetuning</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/andrea-unibg/CLIP%20ResNet101%20Finetuning/runs/ow47emw6' target=\"_blank\">https://wandb.ai/andrea-unibg/CLIP%20ResNet101%20Finetuning/runs/ow47emw6</a>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/andrea-unibg/CLIP%20ResNet101%20Finetuning/runs/ow47emw6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x79b870947ca0>"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["#@title MAIN FUNCTION\n","def main(\n","      batch_size,\n","      device,\n","      learning_rate,\n","      weight_decay,\n","      momentum,\n","      epochs,\n","      people_list,\n","      class_weight,\n","      indices\n","    ):\n","  print(f\"Persone viste in training: {people_list}\")\n","\n","  train_generator = FineTuneDataGenerator(dataset_path, batch_size, preprocess, indices, mode=\"train\")\n","  val_generator = FineTuneDataGenerator(dataset_path, batch_size, preprocess, indices, mode=\"val\")\n","\n","  optimizer = torch.optim.Adam(net.parameters(), lr=args['learning_rate'], weight_decay = weight_decay)\n","  cost_function = torch.nn.BCEWithLogitsLoss(pos_weight=class_weight)\n","\n","  #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1) #ogni 3 epoche riduce LR di un fattore 10\n","\n","  for e in range(epochs):\n","\n","    train_loss, train_accuracy = training_step(net, train_generator, optimizer, cost_function)\n","    val_loss, val_accuracy = test_step(net, val_generator, cost_function)\n","\n","    train_generator.shuffle_replenish_data()\n","\n","    #scheduler.step()\n","\n","    wandb.log({\"Training Loss\": train_loss, \"Training Accuracy\": train_accuracy, \"Validation Loss\": val_loss, \"Validation Accuracy\": val_accuracy})\n","\n","    print('Epoch: {:d}'.format(e+1))\n","    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n","    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n","    print('-----------------------------------------------------')\n","\n","  train_generator.close_file()\n","  val_generator.close_file()\n","  wandb.finish()"],"metadata":{"id":"Y4F0i7i9NbG7","cellView":"form","executionInfo":{"status":"ok","timestamp":1702840948394,"user_tz":-60,"elapsed":205,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["main(**args)"],"metadata":{"id":"VlBl_941NbFQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c937f97d-1440-4257-f3dd-d334a30aba42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Persone viste in training: ['sick', 'long', 'bollinger', 'lieberman']\n"]},{"output_type":"stream","name":"stderr","text":["Train Progress: 2485it [27:55,  1.48it/s]\n","Eval Progress: 622it [02:55,  3.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1\n","\tTraining loss 0.00106, Training accuracy 0.99\n","\tValidation loss 0.00674, Validation accuracy 0.92\n","-----------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Train Progress: 2485it [27:35,  1.50it/s]\n","Eval Progress: 622it [02:57,  3.51it/s]                         \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 2\n","\tTraining loss 0.00097, Training accuracy 0.99\n","\tValidation loss 0.00420, Validation accuracy 0.93\n","-----------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Train Progress: 2485it [27:47,  1.49it/s]\n","Eval Progress: 622it [02:58,  3.49it/s]                         \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 3\n","\tTraining loss 0.00093, Training accuracy 0.99\n","\tValidation loss 0.00369, Validation accuracy 0.95\n","-----------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Train Progress: 2485it [27:46,  1.49it/s]\n","Eval Progress: 622it [02:59,  3.46it/s]                         \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 4\n","\tTraining loss 0.00087, Training accuracy 0.99\n","\tValidation loss 0.00368, Validation accuracy 0.94\n","-----------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Train Progress:  74%|███████▎  | 1830/2484 [20:34<07:22,  1.48it/s]"]}]},{"cell_type":"code","source":["# Save the model's state_dict\n","torch.save(net.state_dict(), '/content/drive/MyDrive/TESI/CODICE/CLIP_Models/ResNet_10_epochs_no_bell.pth')"],"metadata":{"id":"oDHko8ocNbDd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Usare il modello finetunato"],"metadata":{"id":"Umd4ieBTNxGW"}},{"cell_type":"code","source":["import torch\n","\n","# Assuming your CustomCLIP model is already fine-tuned\n","fine_tuned_model = CustomCLIP(num_classes=2)\n","\n","# Load the trained weights\n","fine_tuned_model.load_state_dict(torch.load(\"path/to/your/fine_tuned_model.pth\"))\n","\n","# Set the model to evaluation mode\n","fine_tuned_model.eval()\n","\n","# Assuming 'frames' is a batch of frames that you want to get embeddings for\n","with torch.no_grad():\n","    # Extract embeddings using the CLIP Visual Encoder\n","    embeddings = fine_tuned_model.image_encoder(frames)\n","\n"],"metadata":{"id":"AdzfiplvNwtX"},"execution_count":null,"outputs":[]}]}