{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["gR-O6QXYEyCM"],"mount_file_id":"1fHGkN36gib17ZDHc__AIk35_Y2w7sIZB","authorship_tag":"ABX9TyMi8g8Ob7voVm2aOTZcHf0N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JpuxnxinOIo0","cellView":"form"},"outputs":[],"source":["#@title Load CLIP\n","%%capture\n","!pip -q install ftfy regex tqdm\n","!pip -q install git+https://github.com/openai/CLIP.git\n","#!pip -q install tensorflow_addons torchmetrics\n","\n","import torch\n","import clip\n","import numpy as np\n","from PIL import Image\n","#from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","clip_model, preprocess = clip.load(\"RN101\", device=device)\n","clip_model.eval()"]},{"cell_type":"code","source":["#@title Load Caption Dictionary\n","captions = np.load(\".../LLaVa_fixed_captions.npy\", allow_pickle=True).item()\n","len(captions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"svzDILOmOVJh","executionInfo":{"status":"ok","timestamp":1704987346468,"user_tz":-60,"elapsed":4,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}},"outputId":"ea8b9495-22a7-461a-874d-918820fc5f23","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13732"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["#@title Generate CLIP Text Embeddings\n","from tqdm import tqdm\n","\n","text_emb = {}\n","\n","for key, (speaker, current_label, caption, original_frames_count) in tqdm(captions.items()):\n","\n","    text_tokens = clip.tokenize(caption).cuda()\n","\n","    with torch.no_grad(): text_embeddings = clip_model.encode_text(text_tokens)\n","\n","    text_embeddings = text_embeddings.cpu().detach()\n","\n","    text_emb[key] = (speaker, current_label, text_embeddings, original_frames_count)\n","\n","print(len(text_emb))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RQSg50gMPZ5r","executionInfo":{"status":"ok","timestamp":1704987511882,"user_tz":-60,"elapsed":142086,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}},"outputId":"d1341645-db78-4bec-9fd1-f05e37384bc0","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 13732/13732 [02:21<00:00, 96.77it/s] "]},{"output_type":"stream","name":"stdout","text":["13732\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# Save resulting embeddings\n","np.save('.../ResNet_fixed_captions_emb.npy', text_emb)"],"metadata":{"id":"Yu6vmYATPu90"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## TEST SIMILARITY"],"metadata":{"id":"gR-O6QXYEyCM"}},{"cell_type":"markdown","source":["'No, the person is not talking. He is sitting in front of a microphone, wearing a suit and tie, and looking at the camera.',\n","\"Yes, the person is talking. He is wearing a suit and tie, and he is speaking into a microphone.\" - 92% SIMILARITY"],"metadata":{"id":"DcUj5yHCHoI3"}},{"cell_type":"markdown","source":["\"the person is engaged in a conversation\", \"no one is talking\" - 75% SIMILARITY"],"metadata":{"id":"83rwO0AsHwoD"}},{"cell_type":"code","source":["def cosine_similarity(images_z: torch.Tensor, texts_z: torch.Tensor):\n","  images_z /= images_z.norm(dim=-1, keepdim=True)\n","  texts_z /= texts_z.norm(dim=-1, keepdim=True)\n","\n","  # Valutare la similarità cosinica tra i set di caratteristiche\n","  similarity = (texts_z @ images_z.T)\n","  return similarity.cpu()"],"metadata":{"id":"POBN8oGOBWIa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = clip.tokenize([\"yes\", \"no\"]).to(device)\n","\n","with torch.no_grad():\n","    text_features = clip_model.encode_text(text).float()\n","\n","similarity = cosine_similarity(text_features[0], text_features[1])\n","\n","print(f\"Similarity: {similarity}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h06cM5KYYa4A","executionInfo":{"status":"ok","timestamp":1709470554769,"user_tz":-60,"elapsed":323,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}},"outputId":"8c5dcb11-c9a5-45e0-c210-67d9b9150093"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Similarity: 0.9536035060882568\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-3-db973032768e>:6: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3614.)\n","  similarity = (texts_z @ images_z.T)\n"]}]},{"cell_type":"code","source":["# Prepare the text\n","text = clip.tokenize([\"the person is engaged in a conversation\", \"no one is talking\"]).to(device)\n","\n","with torch.no_grad():\n","    text_features = clip_model.encode_text(text).float()\n","\n","similarity = cosine_similarity(text_features[0], text_features[1])\n","\n","print(f\"Similarity: {similarity}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wIFz1TpuATik","executionInfo":{"status":"ok","timestamp":1704985940273,"user_tz":-60,"elapsed":633,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}},"outputId":"511e2c33-d1d8-4f15-fc9c-69313409d4e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Similarity: 0.7569506764411926\n"]}]},{"cell_type":"markdown","source":["## CREATE FIXED CAPTIONS"],"metadata":{"id":"tsdR0T_sE7kf"}},{"cell_type":"markdown","source":["After checking the similarity of different captions we decided to try out some fixed captions that minimize the similarity:\n"],"metadata":{"id":"vkPsl5PF7YFG"}},{"cell_type":"code","source":["captions = np.load(\"/content/drive/MyDrive/TESI/CODICE/CLIP_Embeddings/LLaVa_long_captions.npy\", allow_pickle=True).item()\n","len(captions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CQPA9xTAE-NM","executionInfo":{"status":"ok","timestamp":1704986017223,"user_tz":-60,"elapsed":607,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}},"outputId":"6720b28c-0b09-42f5-976a-523c27d693df"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13732"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["k = list(captions.keys())[7000]\n","captions[k]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d2uMMOPLFgjU","executionInfo":{"status":"ok","timestamp":1704986126731,"user_tz":-60,"elapsed":2,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}},"outputId":"c20caab9-d2da-4ec6-c214-08bfcc7a4653"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('bell', 1, 'Yes', 10)"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["# Initialize a new dictionary for updated captions\n","updated_captions = {}\n","\n","# Initialize the previous updated caption\n","prev_updated_caption = None\n","\n","for key in list(captions.keys()):\n","    speaker, label, caption, original_frames_count = captions[key]\n","\n","    # If caption starts with \"yes\", change it to \"the person is engaged in a conversation\"\n","    if caption.lower().startswith(\"yes\"):\n","        caption = \"the person is engaged in a conversation\"\n","        prev_updated_caption = caption\n","\n","    # If caption starts with \"no\", change it to \"no one is talking\"\n","    elif caption.lower().startswith(\"no\"):\n","        caption = \"no one is talking\"\n","        prev_updated_caption = caption\n","\n","    # If caption doesn't start with \"yes\" or \"no\", use the previous updated caption\n","    elif prev_updated_caption is not None:\n","        caption = prev_updated_caption\n","\n","    # Store the updated caption in the new dictionary\n","    updated_captions[key] = (speaker, label, caption, original_frames_count)\n","\n","len(updated_captions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5rXQlyAEFhbG","executionInfo":{"status":"ok","timestamp":1704987114416,"user_tz":-60,"elapsed":2,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}},"outputId":"193e748d-7108-4e8d-cc0a-87a80d1265b8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13732"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["k = list(updated_captions.keys())[7000]\n","updated_captions[k]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oh5mHdPIJtFc","executionInfo":{"status":"ok","timestamp":1704987152380,"user_tz":-60,"elapsed":3,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}},"outputId":"b50ed67a-4334-4869-f05e-a7b7f23a05a2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('bell', 1, 'the person is engaged in a conversation', 10)"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["np.save('/content/drive/MyDrive/TESI/CODICE/CLIP_Embeddings/LLaVa_fixed_captions.npy', updated_captions)"],"metadata":{"id":"WSSzBl0MJ01X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These captions will also be encoded into embeddings, like we did earlier in this notebook."],"metadata":{"id":"Cij3wwtw7tgc"}}]}