{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["WofVPCBYeCzK"],"gpuType":"T4","mount_file_id":"1k85A60TOu4Ld8OlzOZjcUsmWX7quto97","authorship_tag":"ABX9TyOY/MhDCPFvrgnbUO+SvEXT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"R5bq7iuhCe7p","cellView":"form"},"outputs":[],"source":["#@title Load CLIP\n","%%capture\n","!pip install ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git\n","import pandas as pd\n","import numpy as np\n","import cv2\n","import os\n","from tqdm import tqdm\n","from PIL import Image\n","from IPython.display import Image as ImagePy, display\n","import torch\n","import warnings\n","warnings.filterwarnings('ignore')\n","import h5py\n","import clip\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","clip_model, preprocess = clip.load(\"RN101\", device=device)\n","clip_model.eval() # per sicurezza"]},{"cell_type":"code","source":["#@title Load Finetuned CLIP Visual Encoder\n","%%capture\n","from torch.nn import functional as F\n","class MLP(torch.nn.Module):\n","    def __init__(self):\n","        super(MLP, self).__init__()\n","        self.fc1 = torch.nn.Linear(512, 64)\n","        self.bn1 = torch.nn.BatchNorm1d(64)\n","        self.fc2 = torch.nn.Linear(64, 64)\n","        self.bn2 = torch.nn.BatchNorm1d(64)\n","        self.fc3 = torch.nn.Linear(64, 1)\n","\n","    def forward(self, x):\n","        x = F.relu(self.bn1(self.fc1(x)))\n","        x = F.relu(self.bn2(self.fc2(x)))\n","        x = self.fc3(x)\n","        return x\n","\n","class CustomCLIP(torch.nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    model = clip_model\n","\n","    self.encoder = model.visual.float()\n","\n","    self.classifier = MLP() # Classifier\n","\n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","    x = self.encoder(x)\n","    x = self.classifier(x)\n","    return x\n","\n","net = CustomCLIP().to(device)\n","checkpoint = torch.load(\"/content/drive/MyDrive/TESI/CODICE/CLIP_Models/no_long_8_epochs\")\n","net.load_state_dict(checkpoint['model_state_dict'])\n","net.eval()"],"metadata":{"id":"pz1QIh-kvOxG","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Generate CLIP embeddings from frames\n","\n","def get_clip_embeddings(frames, finetuned_flag):\n","    if len(frames)!=10:\n","      print(f\"WARNING: get_clip_embeddings received {len(frames)} frames!\")\n","\n","    # Convert the numpy array of frames to a list of PIL Images and convert to RGB if necessary\n","    images = [Image.fromarray(frame) for frame in frames]\n","    images = [image.convert('RGB') if image.mode != 'RGB' else image for image in images]\n","    image_inputs = torch.stack([preprocess(image) for image in images]).to(device)\n","\n","    with torch.no_grad():\n","        if finetuned_flag: embeddings = net.encoder(image_inputs)\n","        else: embeddings = clip_model.encode_image(image_inputs)\n","\n","    # Average the embeddings\n","    video_segment_embedding = embeddings.mean(dim=0)\n","    video_segment_embedding = torch.nn.functional.normalize(video_segment_embedding, p=2, dim=0) # normalize to help the MLP classifier later\n","    return video_segment_embedding.cpu()"],"metadata":{"id":"GdxIhYJKCuwO","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Generate CLIP patch embeddings from frames\n","\n","# I primi 9 Embedding sono le patch, il 10 Ã¨ dell'immagine completa\n","def get_patch_embeddings(frames, finetuned_flag):\n","    if len(frames)!=10:\n","      print(f\"WARNING: get_clip_embeddings received {len(frames)} frames!\")\n","\n","    # Convert the numpy array of frames to a list of PIL Images and convert to RGB if necessary\n","    images = [Image.fromarray(frame) for frame in frames]\n","    images = [image.convert('RGB') if image.mode != 'RGB' else image for image in images]\n","\n","    # Divide each image into 9 non-overlapping patches\n","    patch_size = 224 // 3  # results in 74, the bottom&right-most 2 pixels will be lost\n","    patches = [[image.crop((i * patch_size, j * patch_size, (i + 1) * patch_size, (j + 1) * patch_size)) for i in range(3) for j in range(3)] for image in images]\n","\n","    # Transpose the list of patches so that the same positions are together\n","    patches = list(map(list, zip(*patches)))\n","\n","    # Flatten the list of patches\n","    patches_flat = [patch for image_patches in patches for patch in image_patches]\n","\n","    # Add the original images at the end of the patches_flat list\n","    patches_flat += images\n","\n","    image_inputs = torch.stack([preprocess(image) for image in patches_flat]).to(device)\n","\n","    with torch.no_grad():\n","        if finetuned_flag: embeddings = net.encoder(image_inputs)\n","        else: embeddings = clip_model.encode_image(image_inputs)\n","\n","    # Reshape and average the embeddings\n","    embeddings = embeddings.view(10, len(frames), -1)\n","    video_segment_embedding = embeddings.mean(dim=1)\n","    video_segment_embedding = torch.nn.functional.normalize(video_segment_embedding, p=2, dim=1) # normalize to help the MLP classifier later\n","\n","    return video_segment_embedding.cpu()"],"metadata":{"cellView":"form","id":"uRNEY4N6LJ7H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generate Embeddings from HDF5 dataset"],"metadata":{"id":"WofVPCBYeCzK"}},{"cell_type":"code","source":["# Load dataset\n","!cp \".../Columbia Dataset/Frames_labels_dataset/database.h5\" \"/content/dataset.h5\""],"metadata":{"id":"40TeaHezeFmx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_path = \"/content/database.h5\""],"metadata":{"id":"BoVnXseFfNIZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Check Data\n","print(\"----------------------- FRAMES ---- LABELS\")\n","with h5py.File(dataset_path, 'r') as f:\n","    tot_frames = 0\n","    tot_labels = 0\n","    for p in f.keys():\n","      person = f[p]\n","      group_f = person['frames'].attrs.get('data_length')\n","      group_l = person['labels'].attrs.get('data_length')\n","\n","      tot_frames += group_f\n","      tot_labels += group_l\n","\n","      print(f'Group {p}:      \\t{group_f} \\t\\t{group_l}')\n","\n","    print(f\"TOTAL DATA------------- {tot_frames} --------- {tot_labels}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"GHY73b24eZqm","executionInfo":{"status":"ok","timestamp":1705593977014,"user_tz":-60,"elapsed":488,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}},"outputId":"e80d2781-9315-4f61-addb-aedf2dc8c3f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------- FRAMES ---- LABELS\n","Group bell:      \t37422 \t\t37422\n","Group bollinger:      \t15080 \t\t15080\n","Group lieberman:      \t16400 \t\t16400\n","Group long:      \t29391 \t\t29391\n","Group sick:      \t38526 \t\t38526\n","TOTAL DATA------------- 136819 --------- 136819\n"]}]},{"cell_type":"code","source":["embeddings_dict = {} # Store resulting embeddings"],"metadata":{"id":"jEFl3OOaR5mH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Generate Embeddings from dataset\n","\n","with h5py.File(dataset_path, 'r') as f:\n","  #for speaker in ['bell','sick','long','lieberman','bollinger']:\n","  for speaker in ['bell']:\n","    print(speaker.upper())\n","    start_len = len(embeddings_dict)\n","    person = f[speaker]\n","    frames = person['frames']\n","    labels = person['labels']\n","    frame_index = 0\n","    progress_bar = tqdm(total=len(frames), position=0, leave=False)\n","\n","    # Process frames through 'Frame Interpolation' to create segments of fixed lenght=10\n","    while frame_index < len(frames):\n","        current_label = labels[frame_index]\n","        current_frames = []\n","        original_frames_count = 0\n","        while len(current_frames) < 10 and frame_index < len(frames):\n","            if labels[frame_index] == current_label:\n","                current_frames.append(frames[frame_index])\n","                original_frames_count += 1\n","                frame_index += 1\n","            else:\n","                break\n","        while len(current_frames) < 10:\n","            current_frames += current_frames[:10 - len(current_frames)]\n","        embedding = get_clip_embeddings(np.array(current_frames), True)\n","        key = f\"{speaker}_{frame_index - len(current_frames)}_{frame_index - 1}\"\n","        embeddings_dict[key] = (speaker, current_label, embedding, original_frames_count)\n","        progress_bar.update(10)\n","    progress_bar.close()\n","    end_len = len(embeddings_dict)\n","\n","    print(f\"New Items: {end_len-start_len} \\t TOTAL: {end_len} \\n------------------------------\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gq93PR_-BYTj","executionInfo":{"status":"ok","timestamp":1707238578128,"user_tz":-60,"elapsed":290004,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}},"outputId":"abb55d27-be09-445b-9e6c-984b756cef57","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BELL\n"]},{"output_type":"stream","name":"stderr","text":["                           "]},{"output_type":"stream","name":"stdout","text":["Nuovi aggiunti: 3751 \t TOTALE: 13732 \n","------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}]},{"cell_type":"code","source":["#@title Generate PATCH Embeddings\n","\n","with h5py.File(dataset_path, 'r') as f:\n","  #for speaker in ['bell','sick','long','lieberman','bollinger']:\n","  for speaker in ['lieberman','bollinger']:\n","    print(speaker.upper())\n","    start_len = len(embeddings_dict)\n","    person = f[speaker]\n","    frames = person['frames']\n","    labels = person['labels']\n","    frame_index = 0\n","    progress_bar = tqdm(total=len(frames), position=0, leave=False)\n","\n","    # Process frames through 'Frame Interpolation' to create segments of fixed lenght=10\n","    while frame_index < len(frames):\n","        current_label = labels[frame_index]\n","        current_frames = []\n","        original_frames_count = 0\n","        while len(current_frames) < 10 and frame_index < len(frames):\n","            if labels[frame_index] == current_label:\n","                current_frames.append(frames[frame_index])\n","                original_frames_count += 1\n","                frame_index += 1\n","            else:\n","                break\n","        while len(current_frames) < 10:\n","            current_frames += current_frames[:10 - len(current_frames)]\n","        embedding = get_patch_embeddings(np.array(current_frames), True)\n","\n","        embedding = embedding.view(-1)  # flatten the tensor\n","\n","        key = f\"{speaker}_{frame_index - len(current_frames)}_{frame_index - 1}\"\n","        embeddings_dict[key] = (speaker, current_label, embedding, original_frames_count)\n","        progress_bar.update(10)\n","    progress_bar.close()\n","    end_len = len(embeddings_dict)\n","    print(f\"New Items: {end_len-start_len} \\t TOTAL: {end_len} \\n------------------------------\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AVadpn9fOQLs","outputId":"128f1849-b4f9-4d83-bb7f-b0299acc5db4","cellView":"form","executionInfo":{"status":"ok","timestamp":1707250234020,"user_tz":-60,"elapsed":2310769,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LIEBERMAN\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Nuovi aggiunti: 1643 \t TOTALE: 12193 \n","------------------------------\n","BOLLINGER\n"]},{"output_type":"stream","name":"stderr","text":["                          "]},{"output_type":"stream","name":"stdout","text":["Nuovi aggiunti: 1539 \t TOTALE: 13732 \n","------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}]},{"cell_type":"code","source":["# Save Result\n","np.save('.../CLIP_Embeddings/Finetuned_no_long_patches.npy', embeddings_dict)\n","len(embeddings_dict)"],"metadata":{"id":"iUuueXIAq5Ge","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eebcb806-3dfc-4d63-c485-bf56680cb8c3","executionInfo":{"status":"ok","timestamp":1707250328202,"user_tz":-60,"elapsed":2375,"user":{"displayName":"ANDREA APPIANI","userId":"03631625914545318911"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13732"]},"metadata":{},"execution_count":9}]}]}